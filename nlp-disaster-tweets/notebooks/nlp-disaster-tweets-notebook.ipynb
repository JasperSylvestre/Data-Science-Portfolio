{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c36d6b-ba65-4d6a-9212-db5d34cef61d",
   "metadata": {},
   "source": [
    "# Data Science Portfolio: NLP Disaster Tweets\n",
    "\n",
    "**Author:** Jasper Sylvestre  \n",
    "**Date:** March 13th, 2024\n",
    "\n",
    "**Description:** This notebook showcases NLP and modeling BiLSTM architecture using the data available on __[Kaggle](https://www.kaggle.com/competitions/nlp-getting-started/data)__. A summary and list of conclusions are provided at the end.\n",
    "\n",
    "**Version:** 1.0  \n",
    "**Dependencies:**\n",
    "- python 3.12.2\n",
    "- ipykernel 6.29.3\n",
    "- ipython 8.22.1\n",
    "- jupyter_client 8.6.0\n",
    "- jupyter_core 5.7.1\n",
    "- matplotlib 3.8.3\n",
    "- numpy 1.26.4\n",
    "- pandas 2.2.0\n",
    "- scikit-learn 1.4.1.post1\n",
    "- scipy 1.12.0\n",
    "- seaborn 0.13.2\n",
    "- spacy 3.7.4\n",
    "- torch 2.2.0\n",
    "- torchdata 0.7.1\n",
    "- tqdm 4.66.2\n",
    "\n",
    "**Acknowledgments:**  \n",
    "- Datasets provided by Addison Howard, devrishi, Phil Culliton, and Yufeng Guo.\n",
    "- Inspired by the code from Kaggle user @OmarioVIC found __[here](https://www.kaggle.com/code/omariovic/nlp-disasters-predictions-with-pytorch)__.\n",
    "\n",
    "**License:** MIT License  \n",
    "**Contact:** jesylvestre0@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e6e57-80e9-410f-ba69-244b1c4081de",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Jupyter notebook aims to predict whether or not Tweets correspond to real disasters or not. This will involve forming a submission CSV file with `id` values corresponding to predicted `target` values. The F1 score of the model is the metric being used on the Kaggle page and is the metric we will use here primarily to evaluate the models.\n",
    "\n",
    "The datasets used for modeling in this notebook can be found __[here](https://www.kaggle.com/competitions/nlp-getting-started/dataet)__ and was found on Kaggle. We will begin by importing necessary libraries to conduct data science and machine learning procedures for natural language processing (NLP) and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc59fb4-b13e-4bae-8b5a-94a6cc6ff33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    cohen_kappa_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441e0db-5450-42ab-93ce-e802616c8665",
   "metadata": {},
   "source": [
    "# Reading in the Data and Initial Inspection\n",
    "\n",
    "We will import the datasets that are in the CSV format and are in the \"data\" folder, with separate testing and training datasets. The target values indicate whether a Tweet corresponds to a real disaster, denoted by a 1, and 0 if not. There is no given target column in the testing dataset, and we will try to predict these values for the testing dataset later on. `id` will be kept for the testing dataset since we will need it for the submission CSV file.\n",
    "\n",
    "We will only predict given the text variable as a predictor. Some of the data can be seen. The target values seem to be somewhat balanced for the training dataset. Upon inspection, the datatypes seem to be correct. The datasets seem to be ready for data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193edd88-3079-4b63-8aeb-c84fe528e72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    7613 non-null   object\n",
      " 1   target  7613 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 119.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0                 Just happened a terrible car crash\n",
       "1   2  Heard about #earthquake is different cities, s...\n",
       "2   3  there is a forest fire at spot pond, geese are...\n",
       "3   9           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      3263 non-null   int64 \n",
      " 1   text    3263 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 51.1+ KB\n",
      "Target values for the training dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read data\n",
    "train_path = \"../data/train.csv\"\n",
    "train_df = pd.read_csv(train_path)[[\"text\", \"target\"]]\n",
    "\n",
    "test_path = \"../data/test.csv\"\n",
    "test_df = pd.read_csv(test_path)[[\"id\", \"text\"]]\n",
    "\n",
    "# Examine data\n",
    "display(train_df.head(5))\n",
    "train_df.info()\n",
    "\n",
    "display(test_df.head(5))\n",
    "test_df.info()\n",
    "\n",
    "print(\"Target values for the training dataset.\")\n",
    "display(train_df[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf867554-fdec-40d9-b7ef-9272491426f4",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "We will check for duplicate values, and we can find 92 duplicate observations across `text` and `target` columns. We will remove these observations from the training dataset. Past that, we will check for duplicate observations across the text column with contradictory `target` values. We can find 36 of these observations, which are problematic for training. We will remove these observations as well.\n",
    "\n",
    "We will check for missing variables and can see there are none in the training dataset. Now we will move onto text processing to further prepare the data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8002b5bc-0275-42f9-a448-e389073cf77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 92 duplicate entries across all variables.\n",
      "There are 36 duplicate observations across the `text` column with contradictory `target` values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>#Allah describes piling up #wealth thinking it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244</th>\n",
       "      <td>#foodscare #offers2go #NestleIndia slips into ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>#foodscare #offers2go #NestleIndia slips into ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>CLEARED:incident with injury:I-495  inner loop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4618</th>\n",
       "      <td>CLEARED:incident with injury:I-495  inner loop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>Caution: breathing may be hazardous to your he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>Caution: breathing may be hazardous to your he...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "4290  #Allah describes piling up #wealth thinking it...       0\n",
       "4312  #Allah describes piling up #wealth thinking it...       1\n",
       "4244  #foodscare #offers2go #NestleIndia slips into ...       0\n",
       "4221  #foodscare #offers2go #NestleIndia slips into ...       1\n",
       "2832  .POTUS #StrategicPatience is a strategy for #G...       0\n",
       "2830  .POTUS #StrategicPatience is a strategy for #G...       1\n",
       "4597  CLEARED:incident with injury:I-495  inner loop...       1\n",
       "4618  CLEARED:incident with injury:I-495  inner loop...       0\n",
       "4235  Caution: breathing may be hazardous to your he...       0\n",
       "4232  Caution: breathing may be hazardous to your he...       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing values in this dataset.\n"
     ]
    }
   ],
   "source": [
    "# Show duplicate count\n",
    "print(f\"There are {train_df.duplicated().sum()} duplicate entries across all variables.\")\n",
    "\n",
    "# Remove these duplicates\n",
    "train_df_remove_duplicates = train_df.drop_duplicates()\n",
    "train_df_remove_duplicates\n",
    "\n",
    "# Show non-unique values across the text column with contradictory target values\n",
    "non_unique_values = train_df_remove_duplicates[train_df_remove_duplicates.duplicated(subset=[\"text\"], keep=False)].sort_values(by=\"text\")\n",
    "print(f\"There are {len(non_unique_values)} duplicate observations across the `text` column with contradictory `target` values.\")\n",
    "display(non_unique_values.head(10))\n",
    "\n",
    "# Remove observations with contradictory target values given the same text\n",
    "train_df_cleaned = train_df_remove_duplicates.groupby(\"text\").filter(lambda x: x[\"target\"].nunique() == 1)\n",
    "\n",
    "# Show missing values if there are any, otherwise show there are none\n",
    "if len(train_df_cleaned) == len(train_df_cleaned.dropna()):\n",
    "    print(\"There are no missing values in this dataset.\")\n",
    "else:\n",
    "    print(f\"There are {len(train_df_cleaned) - len(train_df_cleaned.dropna())} missing values in this dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14287d48-98cd-48dd-9eca-63053b7b5d14",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "We will use the `en_core_web_lg` model in spaCy, a large English language model providing word vectors of size 300. These vectors encode semantic information necessary for a model to understand the Tweets.\n",
    "\n",
    "We will make a function called `text_preprocess` to preprocess the text by handling empty strings, removing URLs, removing non-alphabetic characters, removing one-character words, and we can tokenize, lemmatize, and remove stop words, and then return the processed text.\n",
    "\n",
    "We can see an example of the text preprocessing on some of the training data. We will then apply text preprocessing across all the text data for the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4953c0-5c79-48be-a973-7888ffd6f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "VECTOR_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bb241a-1e67-468a-a1f1-309929440788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 deed reason earthquake allah forgive\n",
       "1                forest fire near la ronge sask canada\n",
       "2    resident ask shelter place notify officer evac...\n",
       "3    people receive wildfire evacuation order calif...\n",
       "4    get send photo ruby alaska smoke wildfire pour...\n",
       "5    rockyfire update california hwy close directio...\n",
       "6    flood disaster heavy rain cause flash flooding...\n",
       "7                                       hill fire wood\n",
       "8          emergency evacuation happen building street\n",
       "9                             afraid tornado come area\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define text preprocessing function\n",
    "def text_preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the input text for NLP tasks.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input text to be preprocessed.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    # Handle empty string\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"https?://\\S+\", \" \", text)\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove one-character words\n",
    "    text = \" \".join(word for word in text.split() if len(word) > 1)\n",
    "    \n",
    "    # Tokenize, lemmatize, and remove stop words\n",
    "    doc = nlp(text)\n",
    "    processed_text = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    return \" \".join(processed_text)\n",
    "\n",
    "# Example of text preprocessing\n",
    "display(train_df_cleaned[\"text\"].head(10).apply(text_preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38dae6e2-e4b6-4a19-aeb6-a7dbd4578272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>get send photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>worldnews fall powerline link tram update fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>flip walmart bomb evacuate stay tuned blow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>suicide bomber kill saudi security site mosque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>giant crane hold bridge collapse nearby home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>late home raze northern california wildfire ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7485 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...   \n",
       "1                Forest fire near La Ronge Sask. Canada   \n",
       "2     All residents asked to 'shelter in place' are ...   \n",
       "3     13,000 people receive #wildfires evacuation or...   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...   \n",
       "...                                                 ...   \n",
       "7604  #WorldNews Fallen powerlines on G:link tram: U...   \n",
       "7605  on the flip side I'm at Walmart and there is a...   \n",
       "7606  Suicide bomber kills 15 in Saudi security site...   \n",
       "7608  Two giant cranes holding a bridge collapse int...   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "0                  deed reason earthquake allah forgive  \n",
       "1                 forest fire near la ronge sask canada  \n",
       "2     resident ask shelter place notify officer evac...  \n",
       "3     people receive wildfire evacuation order calif...  \n",
       "4     get send photo ruby alaska smoke wildfire pour...  \n",
       "...                                                 ...  \n",
       "7604  worldnews fall powerline link tram update fire...  \n",
       "7605         flip walmart bomb evacuate stay tuned blow  \n",
       "7606  suicide bomber kill saudi security site mosque...  \n",
       "7608       giant crane hold bridge collapse nearby home  \n",
       "7612  late home raze northern california wildfire ab...  \n",
       "\n",
       "[7485 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond goose flee street save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>earthquake safety los angeles safety fastener ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>storm ri bad hurricane city amp hard hit yard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>green line derailment chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>meg issue hazardous weather outlook hwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>cityofcalgary activate municipal emergency pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0                    Just happened a terrible car crash   \n",
       "1     Heard about #earthquake is different cities, s...   \n",
       "2     there is a forest fire at spot pond, geese are...   \n",
       "3              Apocalypse lighting. #Spokane #wildfires   \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "...                                                 ...   \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...   \n",
       "3259  Storm in RI worse than last hurricane. My city...   \n",
       "3260  Green Line derailment in Chicago http://t.co/U...   \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
       "3262  #CityofCalgary has activated its Municipal Eme...   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "0                             happen terrible car crash  \n",
       "1              hear earthquake different city stay safe  \n",
       "2          forest fire spot pond goose flee street save  \n",
       "3                  apocalypse lighting spokane wildfire  \n",
       "4                    typhoon soudelor kill china taiwan  \n",
       "...                                                 ...  \n",
       "3258  earthquake safety los angeles safety fastener ...  \n",
       "3259  storm ri bad hurricane city amp hard hit yard ...  \n",
       "3260                      green line derailment chicago  \n",
       "3261            meg issue hazardous weather outlook hwo  \n",
       "3262  cityofcalgary activate municipal emergency pla...  \n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply text preprocessing across entire text column\n",
    "\n",
    "# For training dataset\n",
    "train_df_cleaned[\"preprocessed_text\"] = train_df_cleaned[\"text\"].apply(text_preprocess)\n",
    "display(train_df_cleaned[[\"text\", \"preprocessed_text\"]])\n",
    "\n",
    "# For testing dataset\n",
    "test_df[\"preprocessed_text\"] = test_df[\"text\"].apply(text_preprocess)\n",
    "display(test_df[[\"text\", \"preprocessed_text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743deb34-65fe-4527-8369-7677a64a47dd",
   "metadata": {},
   "source": [
    "# Converting Words to Numeric Vectors\n",
    "\n",
    "We have determined that the maximum number of tokens across all preprocessed text observations in the training dataset is 21. Our goal is to convert each line of preprocessed text into an array of numeric vectors, effectively encoding semantic information into numeric form for modeling.\n",
    "\n",
    "In instances where there are fewer than 21 tokens in the observation, we will pad the array with zero vectors. This approach ensures uniformity in the size of our arrays, which is necessary for modeling.\n",
    "\n",
    "We will create a function called `word2vec` that employs a vector size of 300, as mentioned earlier, but it will be made to still work given a different input size if a different spaCy model were chosen. The final array shape will be 21 by 300, encorporating these two constants that will be inputs.\n",
    "\n",
    "With this function, we will process all preprocessed text observations, further preparing the data for modeling with the training dataset and predicting with the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfa3a389-369c-4a96-b6f1-e2a3ec846b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of tokens across all preprocessed text observations in the training dataset: 21\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = max(len(text.split()) for text in train_df_cleaned[\"preprocessed_text\"])\n",
    "print(f\"Maximum number of tokens across all preprocessed text observations in the training dataset: {MAX_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "098da74e-9019-4752-96f0-5288ccc8d30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.52920</td>\n",
       "      <td>-2.29540</td>\n",
       "      <td>-2.14100</td>\n",
       "      <td>-3.44950</td>\n",
       "      <td>-0.13994</td>\n",
       "      <td>-0.78471</td>\n",
       "      <td>0.41509</td>\n",
       "      <td>-2.34060</td>\n",
       "      <td>0.464560</td>\n",
       "      <td>0.81966</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0181</td>\n",
       "      <td>4.9255</td>\n",
       "      <td>3.47730</td>\n",
       "      <td>-0.73864</td>\n",
       "      <td>1.94370</td>\n",
       "      <td>0.91835</td>\n",
       "      <td>-0.62650</td>\n",
       "      <td>-2.93510</td>\n",
       "      <td>3.64080</td>\n",
       "      <td>-0.10550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.52097</td>\n",
       "      <td>3.73210</td>\n",
       "      <td>-0.56785</td>\n",
       "      <td>-0.99742</td>\n",
       "      <td>3.17980</td>\n",
       "      <td>1.70950</td>\n",
       "      <td>3.12540</td>\n",
       "      <td>4.95280</td>\n",
       "      <td>-3.320000</td>\n",
       "      <td>-1.51420</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8541</td>\n",
       "      <td>-2.9473</td>\n",
       "      <td>0.51854</td>\n",
       "      <td>-0.61410</td>\n",
       "      <td>-2.77680</td>\n",
       "      <td>2.64960</td>\n",
       "      <td>2.69140</td>\n",
       "      <td>-1.65630</td>\n",
       "      <td>-2.19470</td>\n",
       "      <td>2.04470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.60911</td>\n",
       "      <td>-2.91400</td>\n",
       "      <td>-3.84390</td>\n",
       "      <td>-1.88390</td>\n",
       "      <td>1.23310</td>\n",
       "      <td>-0.73536</td>\n",
       "      <td>1.76360</td>\n",
       "      <td>2.75140</td>\n",
       "      <td>0.031006</td>\n",
       "      <td>0.09363</td>\n",
       "      <td>...</td>\n",
       "      <td>3.9088</td>\n",
       "      <td>-1.5697</td>\n",
       "      <td>2.67430</td>\n",
       "      <td>4.05320</td>\n",
       "      <td>-2.14300</td>\n",
       "      <td>2.99000</td>\n",
       "      <td>0.93813</td>\n",
       "      <td>-1.39700</td>\n",
       "      <td>0.58375</td>\n",
       "      <td>1.89850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.37075</td>\n",
       "      <td>1.75860</td>\n",
       "      <td>-1.45970</td>\n",
       "      <td>3.69710</td>\n",
       "      <td>0.85235</td>\n",
       "      <td>-0.46995</td>\n",
       "      <td>-2.10470</td>\n",
       "      <td>1.10510</td>\n",
       "      <td>-0.324870</td>\n",
       "      <td>0.82496</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1030</td>\n",
       "      <td>1.0051</td>\n",
       "      <td>-0.14496</td>\n",
       "      <td>-2.35320</td>\n",
       "      <td>-0.94861</td>\n",
       "      <td>1.52070</td>\n",
       "      <td>2.01340</td>\n",
       "      <td>-0.89644</td>\n",
       "      <td>1.29970</td>\n",
       "      <td>-0.92038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.15700</td>\n",
       "      <td>-0.48182</td>\n",
       "      <td>-2.34630</td>\n",
       "      <td>-2.66910</td>\n",
       "      <td>-1.50260</td>\n",
       "      <td>3.71340</td>\n",
       "      <td>0.36540</td>\n",
       "      <td>0.56586</td>\n",
       "      <td>-0.370170</td>\n",
       "      <td>-0.38160</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9691</td>\n",
       "      <td>-1.8302</td>\n",
       "      <td>1.18770</td>\n",
       "      <td>-1.59130</td>\n",
       "      <td>1.21540</td>\n",
       "      <td>-0.70313</td>\n",
       "      <td>0.20679</td>\n",
       "      <td>3.16780</td>\n",
       "      <td>-3.14600</td>\n",
       "      <td>0.80656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3        4        5        6        7    \\\n",
       "0  -2.52920 -2.29540 -2.14100 -3.44950 -0.13994 -0.78471  0.41509 -2.34060   \n",
       "1  -0.52097  3.73210 -0.56785 -0.99742  3.17980  1.70950  3.12540  4.95280   \n",
       "2  -0.60911 -2.91400 -3.84390 -1.88390  1.23310 -0.73536  1.76360  2.75140   \n",
       "3   0.37075  1.75860 -1.45970  3.69710  0.85235 -0.46995 -2.10470  1.10510   \n",
       "4   1.15700 -0.48182 -2.34630 -2.66910 -1.50260  3.71340  0.36540  0.56586   \n",
       "5   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "6   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "7   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "8   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "9   0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "10  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "11  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "12  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "13  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "14  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "15  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "16  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "17  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "18  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "19  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "20  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "\n",
       "         8        9    ...     290     291      292      293      294  \\\n",
       "0   0.464560  0.81966  ...  1.0181  4.9255  3.47730 -0.73864  1.94370   \n",
       "1  -3.320000 -1.51420  ...  5.8541 -2.9473  0.51854 -0.61410 -2.77680   \n",
       "2   0.031006  0.09363  ...  3.9088 -1.5697  2.67430  4.05320 -2.14300   \n",
       "3  -0.324870  0.82496  ...  1.1030  1.0051 -0.14496 -2.35320 -0.94861   \n",
       "4  -0.370170 -0.38160  ...  2.9691 -1.8302  1.18770 -1.59130  1.21540   \n",
       "5   0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "6   0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "7   0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "8   0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "9   0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "10  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "11  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "12  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "13  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "14  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "15  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "16  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "17  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "18  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "19  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "20  0.000000  0.00000  ...  0.0000  0.0000  0.00000  0.00000  0.00000   \n",
       "\n",
       "        295      296      297      298      299  \n",
       "0   0.91835 -0.62650 -2.93510  3.64080 -0.10550  \n",
       "1   2.64960  2.69140 -1.65630 -2.19470  2.04470  \n",
       "2   2.99000  0.93813 -1.39700  0.58375  1.89850  \n",
       "3   1.52070  2.01340 -0.89644  1.29970 -0.92038  \n",
       "4  -0.70313  0.20679  3.16780 -3.14600  0.80656  \n",
       "5   0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "6   0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "7   0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "8   0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "9   0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "10  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "11  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "12  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "13  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "14  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "15  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "16  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "17  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "18  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "19  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "20  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "\n",
       "[21 rows x 300 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def word2vec(text: str,\n",
    "             vector_size: int,\n",
    "             max_len: int) -> np.array(np.float64):\n",
    "    \"\"\"\n",
    "    Convert text to numeric vectors using spaCy's NLP pipeline.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): Input text to be converted to numeric vectors.\n",
    "    - vector_size (int): Size of the vector.\n",
    "    - max_len (int): Size of maximum number of words in text.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray(np.float64): Array of np.float64 vectors indicating numerically encoded semantic information.\n",
    "    \"\"\"\n",
    "    # Process the text using spaCy's NLP pipeline\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize a list to store numeric vectors\n",
    "    num_vectors = []\n",
    "    \n",
    "    # Iterate over tokens in the processed document\n",
    "    for token in doc:\n",
    "        # Append the vector representation of each token to the list\n",
    "        num_vectors.append(token.vector)\n",
    "    \n",
    "    # Pad num vectors with zero vectors if the length is less than max_len\n",
    "    num_vectors += [[0] * vector_size for _ in range(max_len - len(num_vectors))]\n",
    "    \n",
    "    # Convert the list of num vectors to a numpy array\n",
    "    num_vectors_array = np.array(num_vectors)\n",
    "    \n",
    "    return num_vectors_array\n",
    "\n",
    "# Example of word2vec function\n",
    "display(pd.DataFrame(word2vec(train_df_cleaned[\"preprocessed_text\"][0], vector_size=VECTOR_SIZE, max_len=MAX_LEN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944dc5da-23a4-4f72-b136-b57d5527d721",
   "metadata": {},
   "source": [
    "# Form Training, Validation, and Testing Data\n",
    "\n",
    "We will be using the training dataset to form X and y tensors across training and validating tensors for modeling. We will use the testing dataset to form an X tensor that will be predicted on to for the submission predictions.\n",
    "\n",
    "This will involve using the word to numeric vector function above to convert the preprocessed text into numeric vectors for modeling to populate X lists. The y arrays can be populated using the target values of the training dataset. We will convert the X lists into arrays. We can set a random seed for reproducability (this will be `123` for this notebook) for splitting the X and y arrays into correspodning training and validation arrays. 20% of the data will be used for validation and 80% will be used for training. We will keep track of the shapes of each array to make sure there are no errors later on related to shape.\n",
    "\n",
    "We will convert the arrays into tensors. We will maintain keeping track of the shapes of these tensors.\n",
    "\n",
    "Finally, we can convert these tensors into data loaders to be used for modeling and predicting. We will use a batch size of 64. We will maintain keeping track of the shapes with the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a5b33d-439a-416f-a907-8c36eba072ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (7485, 21, 300) | Y Shape: (7485,)\n",
      "X test shape: (3263, 21, 300)\n"
     ]
    }
   ],
   "source": [
    "# Make X variables\n",
    "X = [] # For training and validation datasets\n",
    "X_test = [] # For testing dataset\n",
    "\n",
    "# Populate X with numeric vectors\n",
    "for text in train_df_cleaned[\"preprocessed_text\"]:\n",
    "    X.append(word2vec(text, vector_size=VECTOR_SIZE, max_len=MAX_LEN))\n",
    "\n",
    "# Populate X test with numeric vectors\n",
    "for text in test_df[\"preprocessed_text\"]:\n",
    "    X_test.append(word2vec(text, vector_size=VECTOR_SIZE, max_len=MAX_LEN))\n",
    "\n",
    "# Get y values and convert lists into arrays\n",
    "X_np = np.array(X)\n",
    "X_test_np = np.array(X_test)\n",
    "y_np = train_df_cleaned[\"target\"].values\n",
    "\n",
    "# Keep track of shape of arrays\n",
    "print(f\"X shape: {X_np.shape} | Y Shape: {y_np.shape}\")\n",
    "print(f\"X test shape: {X_test_np.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca7736a3-2261-40cb-ab26-74ab64aaebea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For NumPy arrays:\n",
      "X train shape: (5988, 21, 300) | X val shape: (1497, 21, 300)\n",
      "y train shape: | (5988,) | y val shape: (1497,)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducability\n",
    "RANDOM_SEED = 123\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Form training and validation datasets for X and y variables\n",
    "X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(X_np, y_np, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "# Keep track of shape of arrays\n",
    "print(\"For NumPy arrays:\")\n",
    "print(f\"X train shape: {X_train_np.shape} | X val shape: {X_val_np.shape}\")\n",
    "print(f\"y train shape: | {y_train_np.shape} | y val shape: {y_val_np.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c201d4-d00c-4b1c-a5f4-714de5cd563a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Torch tensors:\n",
      "X train shape: torch.Size([5988, 21, 300]) | X val shape: torch.Size([1497, 21, 300])\n",
      "y train shape: | torch.Size([5988]) | y val shape: torch.Size([1497])\n",
      "X test shape: torch.Size([3263, 21, 300])\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays to tensors\n",
    "X_train = torch.from_numpy(X_train_np.astype(np.float32))\n",
    "X_val = torch.from_numpy(X_val_np.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train_np.astype(np.int64))\n",
    "y_val = torch.from_numpy(y_val_np.astype(np.int64))\n",
    "X_test = torch.from_numpy(X_test_np.astype(np.float32))\n",
    "\n",
    "# Keep track of shape of tensors\n",
    "print(\"For Torch tensors:\")\n",
    "print(f\"X train shape: {X_train.shape} | X val shape: {X_val.shape}\")\n",
    "print(f\"y train shape: | {y_train.shape} | y val shape: {y_val.shape}\")\n",
    "print(f\"X test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669d52cb-a9cb-45d4-b807-f8965ae924c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loader shape: torch.Size([64, 21, 300])\n",
      "Validation data loader shape: torch.Size([64, 21, 300])\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Set batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=BATCH_SIZE)\n",
    "\n",
    "# Keep track of shape\n",
    "print(f\"Training data loader shape: {next(iter(train_loader))[0].shape}\")\n",
    "print(f\"Validation data loader shape: {next(iter(val_loader))[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016384d-33a1-4272-8344-4c7418955536",
   "metadata": {},
   "source": [
    "# Creating the Model\n",
    "\n",
    "We will implement a Bidirectional Long Short-Term Memory (BiLSTM) neural network model using PyTorch. This is an architecture common for sequence-based tasks like NLP. This will be done by implementing a class with input size, hidden size, and number of layers as arguments for initalization. We will also initialize the LSTM layer with these specified pararameters. The linear layer will be defined for classification purposes with double the hidden size as the input because it concatenates the forward and backward hidden states from the bidirectional LSTM. The output size will be 2 since this is binary classification.\n",
    "\n",
    "The forward method defines how the data flows through the network during inference. It will take in the input, and the the hidden state and cell states will be set to zeros. The LSTM layer will process the input and return the output. The hidden state will be extracted from the output and passed through the linear layer to obtain final classification scores.\n",
    "\n",
    "We will initialize two models using device-agnostic code using an input size of the vector size (300) mentioned previously, the hidden size will be 32, and the number of layers will be 5. One model will be used for when we train the model with a patience value of 1 and the other will be when we train the model with a patience value of 5.\n",
    "\n",
    "For training purposes, we will select a criterion and optimizer. The criterion will be cross entropy loss, and the optimizer will be Adam with a learning rate of 0.0001. We will try to use 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a305e45b-b235-4284-826f-3b21f7528357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Linear layer for classification\n",
    "        self.linear = nn.Linear(self.hidden_size * 2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14b2343d-b995-4d4f-880b-a446d3887f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is found to be: 'cpu'\n",
      "\n",
      "Model specifications for training with a patience value of 2: \n",
      "BiLSTM(\n",
      "  (lstm): LSTM(300, 32, num_layers=5, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Model specifications for training with a patience value of 5: \n",
      "BiLSTM(\n",
      "  (lstm): LSTM(300, 32, num_layers=5, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Find device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"The device is found to be: '{device}'\\n\")\n",
    "\n",
    "# Set model and optimizer hyperparameters\n",
    "INPUT_SIZE = VECTOR_SIZE # 300\n",
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Initalize models\n",
    "# Patience value of 2\n",
    "model_1 = BiLSTM(INPUT_SIZE,\n",
    "                 HIDDEN_SIZE,\n",
    "                 NUM_LAYERS).to(device)\n",
    "print(f\"Model specifications for training with a patience value of 2: \\n{model_1}\\n\")\n",
    "\n",
    "# Patience value of 5\n",
    "model_5 = BiLSTM(INPUT_SIZE,\n",
    "                 HIDDEN_SIZE,\n",
    "                 NUM_LAYERS).to(device)\n",
    "print(f\"Model specifications for training with a patience value of 5: \\n{model_5}\")\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizers\n",
    "# Patience value of 2\n",
    "optimizer_1 = torch.optim.Adam(model_1.parameters(),\n",
    "                               lr=LEARNING_RATE)\n",
    "\n",
    "# Patience value of 5\n",
    "optimizer_5 = torch.optim.Adam(model_5.parameters(),\n",
    "                               lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42be3e-5b99-4a8a-b0ea-195da5a77675",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "We will create a function called `train_with_validation` for training the models using a training data loader and predicting with a validation data loader. We will use the model, the training data loader, the validation data loader, the criterion, the optimizer, the device, and the patience value as arguments for the function.\n",
    "\n",
    "During each epoch, the function will iterate over the training data loader, update the model's parameters, and compute the training loss. It will accumulate predictions and ground truth labels for computing the training F1 score. Progress will be tracked using `tqdm` to display the epoch number and loss.\n",
    "\n",
    "After each epoch's training phase, the function will evaluate the model on the validation data loader to compute the validation loss and F1 score. It will accumulate predictions and ground truth labels similarly to the training phase.\n",
    "\n",
    "The average training and validation losses and F1 scores for each epoch will be calculated and stored. Additionally, the function will prints epoch information, training and validation loss, and training and validation F1 scores.\n",
    "\n",
    "If the validation loss does not improve for a certain number of consecutive epochs (the patience value), we will stop the training process. We will use a patience value of 5 for one model and a patience value of 1 for another model. This will be done in an attempt to reduce overfitting.\n",
    "\n",
    "After the completion of all epochs or the training process is stopped, the function will return a tuple containing training losses, validation losses, training F1 scores, and validation F1 scores for further analysis or visualization. We will also return the value of the final epoch to understand if early stopping did occur.\n",
    "\n",
    "We will train both models and can examine that both models did stop early. We will now move onto evaluating the training of the models and the performance of the better model of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ddb7799-3dbf-4275-811c-e16d3eeb5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and validating the model\n",
    "def train_with_validation(model, train_loader, val_loader, epochs, criterion, optimizer, device, patience):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model with validation and compute F1 scores.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to be trained.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for training dataset.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for validation dataset.\n",
    "        epochs (int): Number of epochs for training.\n",
    "        criterion: The loss function used for optimization.\n",
    "        optimizer: The optimizer used for training the model.\n",
    "        device (torch.device): Device on which the data and model will be processed.\n",
    "        patience (int): Number of epochs to wait before early stopping if validation loss does not improve.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing four lists:\n",
    "            - train_losses (list): List of training losses for each epoch.\n",
    "            - val_losses (list): List of validation losses for each epoch.\n",
    "            - train_f1_scores (list): List of training F1 scores for each epoch.\n",
    "            - val_f1_scores (list): List of validation F1 scores for each epoch.\n",
    "            - epoch (int): Final epoch during the training.\n",
    "    \"\"\"\n",
    "    # Initialize train/val losses F1 scores\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_f1_scores = []\n",
    "    val_f1_scores = []\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        \n",
    "        # Training loop with tqdm\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
    "        for samples, labels in progress_bar:\n",
    "            # Put model in training mode\n",
    "            model.train()\n",
    "            \n",
    "            # Put samples and labels to correct device\n",
    "            samples = samples.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(samples)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate the loss for the current epoch\n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            # Accumulate predictions and ground truth labels for training F1 score calculation\n",
    "            train_predictions.extend(predictions.argmax(dim=1).cpu().tolist())\n",
    "            train_targets.extend(labels.cpu().tolist())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        # Calculate average training loss for the epoch\n",
    "        average_train_loss = epoch_train_loss / len(train_loader)\n",
    "        \n",
    "        # Calculate training F1 score\n",
    "        train_f1 = f1_score(train_targets, train_predictions)\n",
    "        \n",
    "        # Validation loop\n",
    "        epoch_val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        \n",
    "        # Perform inference without gradient tracking\n",
    "        with torch.no_grad():\n",
    "            for val_samples, val_labels in val_loader:\n",
    "                # Put model to evaluation mode\n",
    "                model.eval()\n",
    "                \n",
    "                # Put samples and labels to correct device\n",
    "                val_samples = val_samples.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                \n",
    "                # Predict\n",
    "                val_output = model(val_samples)\n",
    "                val_loss = criterion(val_output, val_labels)\n",
    "                epoch_val_loss += val_loss.item()\n",
    "                \n",
    "                val_predictions.extend(val_output.argmax(dim=1).cpu().tolist())\n",
    "                val_targets.extend(val_labels.cpu().tolist())\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        average_val_loss = epoch_val_loss / len(val_loader)\n",
    "        \n",
    "        # Calculate training and validation F1 scores\n",
    "        train_f1 = f1_score(train_targets, train_predictions)\n",
    "        val_f1 = f1_score(val_targets, val_predictions)\n",
    "        \n",
    "        # Save losses and F1 scores for this epoch\n",
    "        train_losses.append(average_train_loss)\n",
    "        val_losses.append(average_val_loss)\n",
    "        train_f1_scores.append(train_f1)\n",
    "        val_f1_scores.append(val_f1)\n",
    "\n",
    "        # Print epoch information including both training and validation loss, and training and validation F1 scores\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {average_train_loss:.4f}, Train F1 Score: {train_f1:.4f}, Val Loss: {average_val_loss:.4f}, Val F1 Score: {val_f1:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if average_val_loss < best_val_loss:\n",
    "            best_val_loss = average_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered. Training stopped at epoch {epoch+1}.\")\n",
    "                break\n",
    "                \n",
    "    return train_losses, val_losses, train_f1_scores, val_f1_scores, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cebc3eef-bb67-4814-867a-60704e0a7f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.6912, Train F1 Score: 0.3660, Val Loss: 0.6839, Val F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Train Loss: 0.6784, Train F1 Score: 0.0000, Val Loss: 0.6598, Val F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Train Loss: 0.5930, Train F1 Score: 0.4335, Val Loss: 0.5472, Val F1 Score: 0.7149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Train Loss: 0.5129, Train F1 Score: 0.7637, Val Loss: 0.5171, Val F1 Score: 0.7375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Train Loss: 0.4619, Train F1 Score: 0.7863, Val Loss: 0.4950, Val F1 Score: 0.7409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Train Loss: 0.4256, Train F1 Score: 0.8029, Val Loss: 0.4920, Val F1 Score: 0.7474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Train Loss: 0.3986, Train F1 Score: 0.8233, Val Loss: 0.4931, Val F1 Score: 0.7479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Train Loss: 0.3738, Train F1 Score: 0.8420, Val Loss: 0.4869, Val F1 Score: 0.7465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Train Loss: 0.3532, Train F1 Score: 0.8568, Val Loss: 0.4995, Val F1 Score: 0.7452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Train Loss: 0.3374, Train F1 Score: 0.8674, Val Loss: 0.5088, Val F1 Score: 0.7490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Train Loss: 0.3220, Train F1 Score: 0.8764, Val Loss: 0.5165, Val F1 Score: 0.7476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Train Loss: 0.3054, Train F1 Score: 0.8846, Val Loss: 0.5260, Val F1 Score: 0.7390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Train Loss: 0.2973, Train F1 Score: 0.8905, Val Loss: 0.5194, Val F1 Score: 0.7407\n",
      "\n",
      "Early stopping triggered. Training stopped at epoch 13.\n"
     ]
    }
   ],
   "source": [
    "# Train and validate the model using patience value of 5\n",
    "train_losses_5, val_losses_5, train_f1_scores_5, val_f1_scores_5, final_epoch_5 = train_with_validation(model=model_5,\n",
    "                                                                                                        train_loader=train_loader,\n",
    "                                                                                                        val_loader=val_loader,\n",
    "                                                                                                        epochs=NUM_EPOCHS,\n",
    "                                                                                                        criterion=criterion,\n",
    "                                                                                                        optimizer=optimizer_5,\n",
    "                                                                                                        device=device,\n",
    "                                                                                                        patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617297f2-96ad-4947-b422-ac4d7a1f47e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.6904, Train F1 Score: 0.3930, Val Loss: 0.6810, Val F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Train Loss: 0.6503, Train F1 Score: 0.1215, Val Loss: 0.5648, Val F1 Score: 0.6828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Train Loss: 0.5182, Train F1 Score: 0.7465, Val Loss: 0.5106, Val F1 Score: 0.7317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Train Loss: 0.4683, Train F1 Score: 0.7747, Val Loss: 0.4922, Val F1 Score: 0.7283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Train Loss: 0.4438, Train F1 Score: 0.7902, Val Loss: 0.4790, Val F1 Score: 0.7374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Train Loss: 0.4225, Train F1 Score: 0.8017, Val Loss: 0.4729, Val F1 Score: 0.7488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:  70%|████████████████████████████████████████▋                 | 66/94 [00:14<00:05,  5.49it/s, loss=0.481]"
     ]
    }
   ],
   "source": [
    "# Train and validate the model using patience value of 1\n",
    "train_losses_1, val_losses_1, train_f1_scores_1, val_f1_scores_1, final_epoch_1 = train_with_validation(model=model_1,\n",
    "                                                                                                        train_loader=train_loader,\n",
    "                                                                                                        val_loader=val_loader,\n",
    "                                                                                                        epochs=NUM_EPOCHS,\n",
    "                                                                                                        criterion=criterion,\n",
    "                                                                                                        optimizer=optimizer_1,\n",
    "                                                                                                        device=device,\n",
    "                                                                                                        patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2121a0-1671-47cb-aabd-e7689527411f",
   "metadata": {},
   "source": [
    "# Evaluating the Model Training\n",
    "\n",
    "We will evaluate the models' training by checking the loss curves and F1 score curves. We will make a function called `loss_f1_score_curve` by using the training losses, the validation losses, the training F1 scores, and validation F1 scores as arguments. We will put these values into dataframes and then plot the curves with accurate labels and axes.\n",
    "\n",
    "We will then use the function to plot these curves. We can see that the model does perform significant better with more training, but there is overfitting after a certain number of epochs where the training loss and F1 score is significantly better than the validation loss and F1 score and the improvement for the validation loss and F1 score begins to plateau while the training loss and F1 score are still getting much better with more training. The validation loss has a tick upwards, so then the training ends for the model with a patience value of 1 due to the early stopping method we implemented. The validation loss continues to not improve for the model with a patience value of 5 until it also is stopped early. The plots' exact nature can vary by randomness each time the model is trained, but the general trends will still be the same as described.\n",
    "\n",
    "Due to the overfitting of the model using a patience value of 5 lessening the final validation loss and F1 score, we will move forward to evaluate further the model with a patience value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0881d-856e-431f-88f4-358f5de4749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting training and validation loss curves and F1 score curves.\n",
    "def loss_f1_score_curve(train_losses, val_losses, train_f1_scores, val_f1_scores, suptitle):\n",
    "    \"\"\"\n",
    "    Plots training and validation loss curves and F1 score curves.\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): List of training losses for each epoch.\n",
    "        val_losses (list): List of validation losses for each epoch.\n",
    "        train_f1_scores (list): List of training F1 scores for each epoch.\n",
    "        val_f1_scores (list): List of validation F1 scores for each epoch.\n",
    "        suptitle (str): Super title above both plots.\n",
    "    \"\"\"\n",
    "    # Form dataframe for train/validation loss\n",
    "    df_loss = pd.DataFrame({\n",
    "        \"Epoch\": list(range(1, len(train_losses) + 1)),\n",
    "        \"Training Loss\": train_losses,\n",
    "        \"Validation Loss\": val_losses\n",
    "    })\n",
    "\n",
    "    # Form dataframe for train/validation F1 score\n",
    "    df_f1_score = pd.DataFrame({\n",
    "        \"Epoch\": list(range(1, len(train_f1_scores) + 1)),\n",
    "        \"Training F1 Score\": train_f1_scores,\n",
    "        \"Validation F1 Score\": val_f1_scores\n",
    "    })\n",
    "\n",
    "    # Set style and color palette\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"bright\")\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.suptitle(suptitle, fontsize=16)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.lineplot(x=\"Epoch\", y=\"Training Loss\", data=df_loss, label=\"Training Loss\")\n",
    "    sns.lineplot(x=\"Epoch\", y=\"Validation Loss\", data=df_loss, label=\"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Loss Curves\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlim(1, len(train_losses))\n",
    "    plt.xticks(np.arange(1, len(train_losses) + 1, 1))\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot F1 score curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.lineplot(x=\"Epoch\", y=\"Training F1 Score\", data=df_f1_score, label=\"Training F1 Score\")\n",
    "    sns.lineplot(x=\"Epoch\", y=\"Validation F1 Score\", data=df_f1_score, label=\"Validation F1 Score\")\n",
    "    plt.title(\"Training and Validation F1 Score Curves\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.ylim(-0.05, 1)\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.xlim(1, len(train_losses))\n",
    "    plt.xticks(np.arange(1, len(train_losses) + 1, 1))\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8da334-ff44-48ea-871b-ad9668e2ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define super title for patience value of 5\n",
    "suptitle_5 = \"For model using patience value of 5\"\n",
    "if final_epoch_5 < NUM_EPOCHS: \n",
    "    suptitle_5 += f\" | Early stopping occurred at final epoch {final_epoch_5}\"\n",
    "\n",
    "# Plot the curves for patience value of 5\n",
    "loss_f1_score_curve(train_losses_5, val_losses_5, train_f1_scores_5, val_f1_scores_5, suptitle_5)\n",
    "\n",
    "# Provide space between plots\n",
    "print()\n",
    "\n",
    "# Define super title for patience value of 1\n",
    "suptitle_1 = \"For model using patience value of 1\"\n",
    "if final_epoch_1 < NUM_EPOCHS: \n",
    "    suptitle_1 += f\" | Early stopping occurred at final epoch {final_epoch_1}\"\n",
    "\n",
    "# Plot the curves for patience value of 1\n",
    "loss_f1_score_curve(train_losses_1, val_losses_1, train_f1_scores_1, val_f1_scores_1, suptitle_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af23a85-4a5a-4f86-83bd-b96d9750a94e",
   "metadata": {},
   "source": [
    "# Evaluating Model Perforance\n",
    "\n",
    "We plot the ROC curve and will find the AUC value. The model will be set to evaluation mode, gradient tracking will be disabled for inference, and we will use the model with a patience value of 1 to get y prediction probabilities. We will compute the ROC curve and calculate the AUC score, and then we will form the curve.\n",
    "\n",
    "We end up with an adequate-looking ROC curve and an AUC score of in the 0.8 to 0.9 range, which will depend slightly on randomness but will be consistent across multiple attempts. This is considered to be a good AUC score for binary classification.\n",
    "\n",
    "We will also calculate the precision, recall, accuracy, Matthew's correlation coefficient, Cohen's Kappa, and specificity. We can see these values are adequate but could still be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc10d6c-3f9c-48c9-b21f-474fad43e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference without gradient tracking\n",
    "with torch.no_grad():\n",
    "    # Set the model to evaluation mode\n",
    "    model_1.eval()\n",
    "    # Get predicted probabilities by applying sigmoid function to model outputs\n",
    "    y_pred_probs = torch.sigmoid(model_1(X_val)).cpu().numpy()\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_val.numpy(), y_pred_probs[:, 1])\n",
    "# Compute AUC score\n",
    "auc = roc_auc_score(y_val.numpy(), y_pred_probs[:, 1])\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"b\", label=f\"AUC = {auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\") # Diagonal reference line\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33ffef-ec10-47a1-a272-fb072c3dbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate the precision, recall, accuracy, Matthew's correlation coefficient, Cohen's Kappa, and specificity\n",
    "# Get y pred from y_pred_probs\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_val, y_pred)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_val, y_pred)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Matthew's correlation coefficient\n",
    "mcc = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "# Cohen's Kappa\n",
    "kappa = cohen_kappa_score(y_val, y_pred)\n",
    "\n",
    "# Specificity\n",
    "TN = ((y_val == 0) & (y_pred == 0)).sum() # True Negatives\n",
    "FP = ((y_val == 0) & (y_pred == 1)).sum() # False Positives\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Display values\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130f520-2957-4bec-865f-35ff0d27dff8",
   "metadata": {},
   "source": [
    "# Continuing to Improve the Model\n",
    "\n",
    "Beyond early stopping that was already performed, several strategies can be employed to potentially enhance the performance of the BiLSTM model. One approach is to experiment with different architectures by adjusting the number of hidden units and layers. Increasing the number of hidden units can allow the model to capture more complex patterns in the data, while adding more layers can enable it to learn hierarchical representations. Additionally, fine-tuning hyperparameters such as the learning rate can significantly impact the model's convergence and generalization ability.\n",
    "\n",
    "Furthermore, incorporating regularization techniques such as dropout or weight decay can help prevent overfitting by introducing noise or penalizing large weights, respectively. \n",
    "\n",
    "Moreover, exploring different optimization algorithms beyond Adam might also yield improvements. Techniques like learning rate scheduling can also be beneficial.\n",
    "\n",
    "Finally, data augmentation methods such as adding noise, perturbing input sequences, or generating synthetic samples can potentially increase the model's robustness and performance on the validation data and ultimately the test data. Experimentation across different data preprocessing techniques, feature engineering strategies, and incorporating domain-specific knowledge can also contribute to enhancing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe44569-494e-4e0d-bd69-11b7b9660763",
   "metadata": {},
   "source": [
    "# Predicting and Creating the Submission CSV\n",
    "\n",
    "Finally, we will predict using the chosen model and the testing dataset that was prepared earlier. We will set the model to evaluation mode, send the testing data to the device, disable gradient-tracking, and find the predictions and convert them to an array. We will display the shape and first 10 predictions to confirm the output is in line with what is expected.\n",
    "\n",
    "We will then make a dataframe for submission, using the `id` column we saved from earlier and the predictions we made. We will display this dataframe to confirm it is in line with what is expected.\n",
    "\n",
    "We will then save the dataframe as a CSV to the \"data\" folder without indexing, and it will be ready for submission in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0c598-b6b0-4880-99fb-127b5b3a029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model_5.eval()\n",
    "\n",
    "# Move the test data to the device\n",
    "X_test = X_test.to(device)\n",
    "\n",
    "# Perform inference without gradient tracking\n",
    "with torch.no_grad():\n",
    "    # Make predictions\n",
    "    y_pred = model_5(X_test.squeeze(1)).argmax(dim=1)\n",
    "    \n",
    "    # Convert predictions to numpy array\n",
    "    predictions = y_pred.cpu().numpy()\n",
    "\n",
    "# Show shape of predictions\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"First 10 predictions: {predictions[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edecf9-53f4-4681-ab4b-c27ca1329fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for submission\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"target\": predictions\n",
    "})\n",
    "\n",
    "# Display the dataframe\n",
    "display(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55e2c8-7d77-4fab-b1cd-9ba24422b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file without including the index\n",
    "submission_path = \"../data/submission.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b7b2c-41df-4893-9a24-292856c9fc24",
   "metadata": {},
   "source": [
    "# Summary and Conclusions\r\n",
    "\r\n",
    "This notebook focuses on Natural Languagp Processing (NLP) using a Bidirectional Long Short-Term Memory (BiLSTM) neural network architecture to predict whether tweets correspond to real disasters. The notebook utilizes datasets from Kaggle and employs various data preprocessing techniques, including text cleaning and converting words to numeric vectors. The model is trained using PyTorch, and efforts are made to mitigate overfitting through early stopping. The model's performance is evaluated using metrics such as F1 score, ROC curve, and AUC score. Finally, predictions are made on the testing dataset, and a submission CSV file is created for Kaggle.\r\n",
    "\r\n",
    "## Conclusions\r\n",
    "\r\n",
    "Here is a list of conclusions based on different steps taken throughout the notebook:\r\n",
    "\r\n",
    "1. **Data Preprocessing**: The data underwent cleaning processes such as removing duplicates and handling missing values. Text preprocessing techniques were done primarily using spaCy's `en_core_web_lg` model to convert text data to arrays of numerical vectors. Data is separated into X and y training, validation, and testing datasets, except there are no y values for the testing dataset. Datasets are converted into Torch tensors and into data loaders when needed.\r\n",
    "2. **Model Architecture**: A BiLSTM neural network model was implemented using PyTorch, suitable for sequence-based tasks like NLP. Two models were trained with different patience values for early stopping to mitigate overfitting.\r\n",
    "3. **Model Training**: Training was conducted using a custom function that iteratively updates the model parameters while monitoring loss and F1 score. Early stopping was implemented to prevent overfitting.\r\n",
    "4. **Model Evaluation**: The trained model's performance was evaluated using various metrics including F1 score, ROC curve, and AUC score. While the AUC score indicated good performance, some metrics still indicated that improvement could be made.\r\n",
    "5. **Future Improvements**: Suggestions for enhancing model performance were made including experimenting with different architectures, hyperparameter tuning, incorporating regularization techniques, and exploring alternative optimization algorithms.\r\n",
    "6. **Submission**: Predictions were made on the testing dataset, and a submission dataframe and CSV file was prepared for Kaggle competition, showcasing the model's practical application.\r\n",
    "\r\n",
    "Overall, this notebook explored using a BiLSTM neural network for NLP to predict real disasters from tweets. The notebook employed data pre-processing, PyTorch model training with early stopping, and various evaluation metrics. While the model showed promise (good AUC score), there's room for improvement through architecture exploration, hyperparameter tuning, and alternative techniques. Finally, the model was used to form a submission CSV file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
