{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c043af81-ad26-4265-a886-257d6542cb3b",
   "metadata": {},
   "source": [
    "# Data Science Portfolio: Road Accident Analysis - EDA and Data Cleaning\n",
    "\n",
    "**Author:** Jasper Sylvestre  \n",
    "**Date:** March 4, 2024\n",
    "\n",
    "**Description:** This notebook showcases exploratory data analysis and data cleaning on road accident data available on __[Kaggle](https://www.kaggle.com/datasets/farshidbahrami021/road-accident-dataset)__. A summary a list of conclusions are provided at the end. Note that this data set was randomly generated and is purely for learning purposes.\n",
    "\n",
    "**Version:** 1.0  \n",
    "**Dependencies:**\n",
    "- Python 12.2\n",
    "- Pandas 2.2.0\n",
    "- Matplotlib 3.8.3 \n",
    "- Seaborn 0.13.2\n",
    "- scikit-learn 1.4.1\n",
    "- scipy 1.12.0\n",
    "- statmodels 0.14.1\n",
    "\n",
    "**Acknowledgments:**  \n",
    "- Data set provided by Farshid Bahrami on Kaggle\n",
    "\n",
    "**License:** MIT License  \n",
    "**Contact:** jesylvestre0@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e43c95-2ce7-45a7-a02e-4273a5a9608b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This Jupyter notebook aims to perform Exploratory Data Analysis (EDA) and data cleaning on road accident data. The data set used here contains information about road accidents and serves as a foundation for deriving insights and building predictive models. This data set can be found __[here](https://www.kaggle.com/datasets/farshidbahrami021/road-accident-dataset)__ and was found on Kaggle. We can begin by importing necessary libraries to conduct data science procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce2e5c-7564-4cb4-9fca-9f8ef92a171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2_contingency, zscore\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a5ad8-a17e-40b9-8973-c716e6575f47",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Next, we can load in the data. The file is in the same folder as the notebook and is in the \"xlsx\" format, only containing one sheet. After reading in the data, we will examine the first five rows to begin to understand its structure. We can also examine the number of observations and total number of columns. We can see that there are 1610 observations and 23 columns.\n",
    "\n",
    "There is also information given on the Kaggle page on this data set, which also confirms there are supposed to be 1610 observations. Descriptions are given for each column as well on the Kaggle page. It is also noted that some information has also been removed randomly to simulate real-world cases where some data will be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5720f7-bd78-4526-b9e4-413590edc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "file_path = \"road_accident_data.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Examine data\n",
    "print(f\"This data set has: {len(df)} observations and {len(df.columns)} columns\\n\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97d346-c0ba-40d3-814e-a1787460472c",
   "metadata": {},
   "source": [
    "## Working with Time Variables\n",
    "\n",
    "If we want to work with the `Date` and `Time_of_Day` variables for modeling, graphing, and understanding more simply, we want to convert this time data to time in numeric form. For `Date`, this data should first be converted to Unix time. Unix time represents the number of seconds that have elapsed since the Unix epoch (January 1, 1970). We can then start the data at 0 for the beginning and put the `Date` data into the number of days since the starting time, and this time is found to be 12:44:44 AM on January 1st, 2024.  We can also change the name of the `Date` column to match this change.\n",
    "\n",
    "`Time_of_Day` can more simply just be converted into hours past since midnight. This will be done before investigating the data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c5689-0fa0-4502-97b3-4c19e606af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Time Variables\n",
    "\n",
    "## Display start time\n",
    "print(f\"The starting time is: {df[\"Date\"].min()}\\n\")\n",
    "\n",
    "## Convert Date to Unix time in seconds\n",
    "df[\"Date\"] = df[\"Date\"].astype(\"int64\") // 10**9\n",
    "df[\"Date\"] = (df[\"Date\"] - df[\"Date\"].min()) / (24*60*60)\n",
    "\n",
    "## Rename Date\n",
    "df.rename(columns={\"Date\": \"Days_Since_Start\"}, inplace=True)\n",
    "\n",
    "## Convert Time_of_Day to time in days\n",
    "df[\"Time_of_Day\"] = pd.to_timedelta(df[\"Time_of_Day\"]).dt.total_seconds() / (60*60)\n",
    "\n",
    "## Display changed results\n",
    "display(df[[\"Days_Since_Start\", \"Time_of_Day\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb2549-f9f5-4620-8a79-7b6010505b8e",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "\n",
    "To gain deeper insights into the data set, we can examine the data set by examining the data types of each column. 10 of the columns are `object` variables, 3 are `int64`, and 10 are `float64`. This understanding will help us structure our analysis effectively.\n",
    "\n",
    "For the numeric variables, we can quickly get an idea of the distributions for each one as well by showing their descriptive statistics. For `object`-type variables, we can gain additional insights by displaying their unique values and the total count of distinct values per column, not including missing values.\n",
    "\n",
    "Additionally, we can calculate the total number of missing values per column. We can see that only six variables contain missing vales, four being numeric and the other two are categorical. This discrepancy in data types for missing values suggests the need for multiple methods of missing data imputation in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065d35e-8e2e-4d3f-a2d9-35cedb87711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the data\n",
    "\n",
    "## Data types for variables\n",
    "print(\"Data types of variables:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "## Descriptive statistics for numeric variables\n",
    "print(\"\\nDescriptive statistics for numeric variables:\")\n",
    "numeric_stats = df.describe()\n",
    "display(numeric_stats)\n",
    "\n",
    "## Get unique values for each object variable\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == \"object\":\n",
    "        unique_values = df[column].dropna().unique()\n",
    "        print(f\"\\nThere are {len(unique_values)} unique values in column '{column}':\")\n",
    "        print(unique_values)\n",
    "\n",
    "## Missing values per column\n",
    "print(\"\\nMissing value counts per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648cfec9-0a96-41fd-99ed-47e3fbc0b413",
   "metadata": {},
   "source": [
    "## Handling missing data\n",
    "\n",
    "Simply removing all observations including missing data would reduce the total number of observations to just 862, significantly reducing the amount of data there is to work with. If the absence of values actually held context-specific significance — such as unrecorded vehicle speeds possibly correlating with severe accidents for whatever reason — a more thoughtful approach to handling them could be warranted. Currently, we can construct a new data frame, `df_imputed`, and address missing values by replacing missing numeric data with the median and missing string values with the mode. We can also verify this correctly led to all observations now having no missing values.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131d61c-6c29-4468-abf4-f311fe5013d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing data\n",
    "\n",
    "## Print number of observations if all rows with missing data were removed\n",
    "print(f\"Number of observations if all rows with missing data were removed from `df`: {len(df.dropna())}\")\n",
    "\n",
    "## Create a new DataFrame with imputed values\n",
    "df_imputed = df.copy()\n",
    "\n",
    "## Impute missing values for numeric columns with median\n",
    "numeric_columns = df_imputed.select_dtypes(include=np.number).columns\n",
    "for col in numeric_columns:\n",
    "    df_imputed.fillna({col: df_imputed[col].median()}, inplace=True)\n",
    "\n",
    "## Impute missing values for string columns with mode\n",
    "string_columns = df_imputed.select_dtypes(include=\"object\").columns\n",
    "for col in string_columns:\n",
    "    df_imputed.fillna({col: df_imputed[col].mode()[0]}, inplace=True)\n",
    "\n",
    "## Verify imputation was performed successfully\n",
    "print(f\"Number of observations if all rows missing data were removed from `df_imputed`: {len(df_imputed.dropna())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003552f7-4cbe-4227-b6a2-541ea38016e5",
   "metadata": {},
   "source": [
    "## Visualizing Numeric Variables\n",
    "\n",
    "From the Kaggle page for this data set, the ID variable is merely a nominal numeric identifier, incrementing by 1 for each new record, starting at 165433. Therefore, further analysis of this variable is unnecessary. This is verified through code below.\n",
    "\n",
    "We can visualize the remaining numerical variables using histograms along with a smoothed kernel. For integer variables indicating the number of vehicles involved and the number of casualties, a kernel is unnecessary due to the small number of possible values: 5 and 4 total possible values, respectively. For all values, kernel or not, we can examine that the values are relatively balanced across the center. There does not seem to be any skewness or extreme values for any numeric variable. For speed limit, road width, vehicle speed, and time taken for emergency response, the values do seem to be more often around the center. These were also all the variables where missing values were imputed for the median, so this is somewhat expected, and perhaps a different method of imputation should be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44d439-8066-4ff5-9f42-8841cb9c84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Numeric Variables\n",
    "\n",
    "## Verify ID is not necessary to track.\n",
    "if list(df_imputed[\"ID\"]) == list(range(165433, 165433+1610)):\n",
    "    print(\"The ID column is equivalent to a list of 1610 numbers starting at 165433 incrementing by 1 each time.\\n\")\n",
    "\n",
    "## Get non-ID numeric columns\n",
    "numeric_columns = df_imputed.drop(columns=\"ID\").select_dtypes(include=np.number).columns\n",
    "\n",
    "## Set up the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "## Create subplots based on the number of numeric variables\n",
    "fig, axes = plt.subplots(nrows=len(numeric_columns), figsize=(10, 80))\n",
    "\n",
    "## Set the overall title for the figure\n",
    "fig.suptitle(\"Distributions of Numerical Variables Visualized with Histograms\", fontsize=20, y=0.89)\n",
    "\n",
    "## Iterate through each numeric column and plot a histogram\n",
    "for idx, column in enumerate(numeric_columns):\n",
    "    ## If the column is integer, use integer bins and do not use kernel, otherwise find bins automatically and use kernel\n",
    "    if df_imputed[column].dtype == \"int64\":\n",
    "        sns.histplot(data=df_imputed[column],\n",
    "                     ax=axes[idx],\n",
    "                     bins=range(int(df_imputed[column].min()),\n",
    "                                int(df_imputed[column].max()) + 2),\n",
    "                     kde=False,\n",
    "                     color=\"forestgreen\",\n",
    "                     linewidth=2)\n",
    "        axes[idx].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    else:\n",
    "        sns.histplot(data=df_imputed[column],\n",
    "                     ax=axes[idx],\n",
    "                     bins=\"auto\",\n",
    "                     kde=True,\n",
    "                     color=\"forestgreen\",\n",
    "                     linewidth=2)\n",
    "    \n",
    "    axes[idx].set_title(f\"'{column}'\")\n",
    "    axes[idx].set_xlabel(\"\")\n",
    "\n",
    "## Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9783ea-60dc-43b1-bc5b-6b12c76e05a3",
   "metadata": {},
   "source": [
    "### Visualizing Numeric Variables continued\n",
    "\n",
    "We can see that the distributions with the missing data still being contained are much more balanced. This implies that this method of data imputation does seem to at least visually disrupt the distributions of the data. The numerical data all seems to be roughly uniformly distributed with some patterns, e.g. speed limit values are more often being found at multiples of 10 or at 65, likely since these are just more common speed limits. We will now use a different form of data imputation to correct this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d7a02-4c55-4d5b-8c60-667c2c4b2b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Numeric Variables continued\n",
    "\n",
    "## Get numeric columns that had no missing values imputed\n",
    "numeric_columns = df[[\"Speed_Limit\", \"Road_Width\", \"Vehicle_Speed\", \"Time_Taken_for_Emergency_Response\"]].columns\n",
    "\n",
    "## Set up the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "## Create subplots based on the number of numeric variables\n",
    "fig, axes = plt.subplots(nrows=len(numeric_columns), figsize=(10, 22))\n",
    "\n",
    "## Set the overall title for the figure\n",
    "fig.suptitle(\"Distributions of Numerical Variables Visualized with Histograms\\nOnly for Variables with Missing Values\", fontsize=20, y=0.92)\n",
    "\n",
    "## Iterate through each numeric column and plot a histogram\n",
    "for idx, column in enumerate(numeric_columns):\n",
    "    sns.histplot(data=df[column],\n",
    "                     ax=axes[idx],\n",
    "                     bins=\"auto\",\n",
    "                     kde=True,\n",
    "                     color=\"forestgreen\",\n",
    "                     linewidth=2)\n",
    "    \n",
    "    axes[idx].set_title(f\"'{column}'\")\n",
    "    axes[idx].set_xlabel(\"\")\n",
    "\n",
    "## Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc5bb6f-c416-4edc-9bf4-ac8b49937bc8",
   "metadata": {},
   "source": [
    "### Visualizing Numeric Variables continued\n",
    "\n",
    "We will use interpolation to impute the data instead and then revisualize the data with imputed values. This seems to have much more balanced results. Now all the numeric data seems to be roughly uniformly distributed whether the missing values are imputed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb850cc7-4dbd-46ca-8e4a-ad52b247798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Numeric Variables continued\n",
    "\n",
    "## Fill in missing values\n",
    "df_interp = df.copy()\n",
    "\n",
    "### Impute missing values for categorical columns with mode\n",
    "string_columns = df_interp.select_dtypes(include=\"object\").columns\n",
    "for col in string_columns:\n",
    "    df_interp.fillna({col: df_interp[col].mode()[0]}, inplace=True)\n",
    "    \n",
    "### Use interpolation\n",
    "### Suppress irrelevant warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df_interp = df_interp.interpolate(method=\"linear\")\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "### Verify imputation was performed successfully\n",
    "print(f\"Number of observations if all rows missing data were removed from `df_interp`: {len(df_interp.dropna())}\\n\")\n",
    "\n",
    "## Get numeric columns\n",
    "numeric_columns = df_interp[[\"Speed_Limit\", \"Road_Width\", \"Vehicle_Speed\", \"Time_Taken_for_Emergency_Response\"]].columns\n",
    "\n",
    "## Set up the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "## Create subplots based on the number of numeric variables\n",
    "fig, axes = plt.subplots(nrows=len(numeric_columns), figsize=(10, 22))\n",
    "\n",
    "## Set the overall title for the figure\n",
    "fig.suptitle(\"Distributions of Numerical Variables Visualized with Histograms\\nWith Reimputed Data\", fontsize=20, y=0.92)\n",
    "\n",
    "## Iterate through each numeric column and plot a histogram\n",
    "for idx, column in enumerate(numeric_columns):\n",
    "    sns.histplot(data=df_interp[column],\n",
    "                     ax=axes[idx],\n",
    "                     bins=\"auto\",\n",
    "                     kde=True,\n",
    "                     color=\"forestgreen\",\n",
    "                     linewidth=2)\n",
    "    \n",
    "    axes[idx].set_title(f\"'{column}'\")\n",
    "    axes[idx].set_xlabel(\"\")\n",
    "\n",
    "## Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58edfb83-a22f-46f0-b93c-32bc92fe0394",
   "metadata": {},
   "source": [
    "## Visualizing Categorical Variables\n",
    "\n",
    "We notice a similar problem here. Data imputation of the mode has seemed to cause spikes in certain parts of the data for weather conditions and type of junction, both where there was missing data. This could indicate a different method of imputation should be taken here. Past that, the state does vary quite a bit. This could indicate something based on exactly what this data means, e.g. if these were all the car accidents in the U.S. in all 50 states up to a certain point in 2024, but it is currently unknown if this has any significance given the Kaggle page does not specify. For all other variables, however, the data is roughly balanced to resemble an approximate uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b670b1-fadc-4d8a-a5ec-aaa9880d2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Categorical Variables\n",
    "\n",
    "## Get categorical columns\n",
    "categorical_columns = df_interp.select_dtypes(include=\"object\").columns\n",
    "\n",
    "## Set up the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "## Create subplots based on the number of categorical variables\n",
    "fig, axes = plt.subplots(nrows=len(categorical_columns),\n",
    "                         figsize=(12, 40),\n",
    "                         gridspec_kw={\"height_ratios\": [3, 1, 1, 1, 1, 1, 1, 1, 1, 1]})\n",
    "\n",
    "## Set the overall title for the figure\n",
    "fig.suptitle(\"Distribution of Categorical Variables Visualized with Bar Graphs\", fontsize=20, y=0.897)\n",
    "\n",
    "## Iterate through each categorical column and plot a count plot\n",
    "for idx, column in enumerate(categorical_columns):\n",
    "    sns.countplot(data=df_interp,\n",
    "                  y=column,\n",
    "                  hue=column,\n",
    "                  ax=axes[idx],\n",
    "                  palette=\"pastel\",\n",
    "                  legend=False)\n",
    "    axes[idx].set_title(f\"'{column}'\")\n",
    "    axes[idx].set_xlabel(\"\")\n",
    "    axes[idx].set_ylabel(\"Frequency\")\n",
    "\n",
    "## Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbe6aa-b264-4ce1-ba1d-fb880bab1e30",
   "metadata": {},
   "source": [
    "## Visualizing Categorical Variables continued\n",
    "\n",
    "We can see, again, that using a highly simple method of imputation is having the drawback of causing a false spike in the distributions of the graphs where none is naturally found. We will now use a different method of data imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c6544-0eb7-4f6c-b454-cb9fc6d12607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Categorical Variables continued\n",
    "\n",
    "## Get categorical columns that had no missing values imputed\n",
    "categorical_columns = df[[\"Weather_Conditions\", \"Type_of_Junction\"]].columns\n",
    "\n",
    "## Set up the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "## Create subplots based on the number of categorical variables\n",
    "fig, axes = plt.subplots(nrows=len(categorical_columns),\n",
    "                         figsize=(12, 10))\n",
    "\n",
    "## Set the overall title for the figure\n",
    "fig.suptitle(\"Distribution of Categorical Variables Visualized with Bar Graphs\\nOnly for Variables with Missing Values\", fontsize=20, y=0.965)\n",
    "\n",
    "## Iterate through each categorical column and plot a count plot\n",
    "for idx, column in enumerate(categorical_columns):\n",
    "    sns.countplot(data=df,\n",
    "                  y=column,\n",
    "                  hue=column,\n",
    "                  ax=axes[idx],\n",
    "                  palette=\"pastel\",\n",
    "                  legend=False)\n",
    "    axes[idx].set_title(f\"'{column}'\")\n",
    "    axes[idx].set_xlabel(\"\")\n",
    "    axes[idx].set_ylabel(\"Frequency\")\n",
    "\n",
    "## Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d91f49-16f2-4cd5-aa47-58231fd26215",
   "metadata": {},
   "source": [
    "## Visualizing Categorical Variables continued\n",
    "\n",
    "Once again, it is evident that using a simplistic imputation method results in an artificial spike in the distribution graphs where none naturally exists. We will now adopt an alternative approach to data imputation using a random forest classifier. This seems to have significantly improved the overall balance of the data, and we have effectively addressed all missing values to a satisfactory extent for now.\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ee8f1-70a3-49b8-a707-900c0061b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Numeric Variables continued\n",
    "\n",
    "## Add seed for reproducability\n",
    "RANDOM_SEED = 123\n",
    "\n",
    "## Add back in missing values\n",
    "df_interp[\"Weather_Conditions\"] = df[\"Weather_Conditions\"]\n",
    "df_interp[\"Type_of_Junction\"] = df[\"Type_of_Junction\"]\n",
    "\n",
    "## Fill in missing values using random forests\n",
    "missing_columns = [\"Weather_Conditions\", \"Type_of_Junction\"]\n",
    "categorical_columns = [\"State\", \"Day_of_Week\", \"Road_Conditions\", \"Light_Conditions\", \"Type_of_Road\", \"Type_of_Accident\", \"Vehicle_Type\", \"Driver_Age_Group\"] # Cannot use for random forests\n",
    "complete_df = df_interp.dropna(subset=missing_columns).drop(columns=categorical_columns)\n",
    "incomplete_df = df_interp[df_interp.isnull().any(axis=1)].drop(columns=categorical_columns)\n",
    "\n",
    "## Get training and test data\n",
    "X_train = complete_df.drop(missing_columns, axis=1)\n",
    "y_train = complete_df[missing_columns]\n",
    "X_test = incomplete_df.drop(missing_columns, axis=1)\n",
    "\n",
    "## Train RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100,\n",
    "                                       random_state=RANDOM_SEED)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "## Predict missing values\n",
    "predicted_values = rf_classifier.predict(X_test)\n",
    "\n",
    "## Update the DataFrame with predicted values\n",
    "incomplete_df[missing_columns] = predicted_values\n",
    "df_imputed_2 = pd.concat([complete_df, incomplete_df])\n",
    "categorical_columns.append(\"ID\") # Use as key for adding back categorical variables\n",
    "df_imputed_2 = pd.merge(df_imputed_2, df_interp[categorical_columns], on=\"ID\")\n",
    "\n",
    "### Verify imputation was performed successfully\n",
    "print(f\"Number of observations if all rows missing data were removed from `df_imputed_2`: {len(df_imputed_2.dropna())}\\n\")\n",
    "\n",
    "## Get categorical columns\n",
    "categorical_columns = df_imputed_2[[\"Weather_Conditions\", \"Type_of_Junction\"]].columns\n",
    "\n",
    "## Set up the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "## Create subplots based on the number of categorical variables\n",
    "fig, axes = plt.subplots(nrows=len(categorical_columns),\n",
    "                         figsize=(12, 10))\n",
    "\n",
    "## Set the overall title for the figure\n",
    "fig.suptitle(\"Distribution of Categorical Variables Visualized with Bar Graphs\\nWith Reimputed Data\", fontsize=20, y=0.965)\n",
    "\n",
    "## Iterate through each categorical column and plot a count plot\n",
    "for idx, column in enumerate(categorical_columns):\n",
    "    sns.countplot(data=df_imputed_2,\n",
    "                  y=column,\n",
    "                  hue=column,\n",
    "                  ax=axes[idx],\n",
    "                  palette=\"pastel\",\n",
    "                  legend=False)\n",
    "    axes[idx].set_title(f\"'{column}'\")\n",
    "    axes[idx].set_xlabel(\"\")\n",
    "    axes[idx].set_ylabel(\"Frequency\")\n",
    "\n",
    "## Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c348d-2030-4992-84e1-9833c1106973",
   "metadata": {},
   "source": [
    "## Checking For Outliers\n",
    "\n",
    "Only using visual inspection, there do not appear to be any outliers among the numeric variables. We can analyze this further by examining potential outliers not seen in plots using Z-score method, a robut technique for detecting outliers. Applying this method, no outliers are found among the numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa862746-f2b9-49c4-812e-19616078f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics\n",
    "numeric_variables = df.select_dtypes(include=np.number)\n",
    "\n",
    "# Check for outliers\n",
    "for col in numeric_variables:\n",
    "    z_scores = zscore(df[col])\n",
    "    outlier_indices = np.where(np.abs(z_scores) > 3)[0]\n",
    "    print(f\"There are {len(outlier_indices)} outliers in the '{col}' variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae148696-90f8-48bc-84d2-ebe30639eb94",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "The data appears to be clean without any noticable errors, missing values have been handled, and there do not seem to be significant outliers. This leaves data transformation to be done.\n",
    "\n",
    "Regarding numerical data, it will be standardized into its own data frame `df_num_std` and `ID` will be kept the same to be used as a key for potential data merges. The data itself will be ready for a correlation matrix.\n",
    "\n",
    "For categorical variables, one hot encoding will be performed into its own data frame `df_var_encoded` and `ID` will be kept similarly. This will lead to the creation of approximately 100 columns in total, making braod analysis difficult to perform but useful for modeling with. `df_var` will also be kept for bivariate analysis later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184ca5b-4981-413b-98ec-fcd716ba4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "\n",
    "## Prepare numeric data\n",
    "df_num = df_imputed_2.select_dtypes(include=np.number)\n",
    "\n",
    "## Calculate mean and standard deviation for each column\n",
    "mean_values = df_num.mean()\n",
    "std_values = df_num.std()\n",
    "\n",
    "## Keep ID variable the same\n",
    "mean_values[\"ID\"] = 0\n",
    "std_values[\"ID\"] = 1\n",
    "\n",
    "## Standardize each column\n",
    "df_num_std = (df_num - mean_values) / std_values\n",
    "\n",
    "## Display\n",
    "display(df_num_std)\n",
    "\n",
    "# One hot encode categorical variables\n",
    "\n",
    "## Prepare categorical data\n",
    "df_var = df_imputed_2.select_dtypes(include=[\"int64\", \"object\"]).drop(columns=[\"Num_Casualties\", \"Num_Vehicles_Involved\"]) # Keep ID but not other int64 variables\n",
    "\n",
    "## One hot encode\n",
    "df_var_encoded = pd.get_dummies(df_var, dtype=int)\n",
    "\n",
    "## Display\n",
    "display(df_var_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ec834-ba3c-43a4-b838-015b2cc994ba",
   "metadata": {},
   "source": [
    "## Bivariate Analysis - Correlation Matrix\n",
    "\n",
    "We can use the standardized numerical data to construct a correlation matrix and heatmap. This is done to examine potential linear correlations between numeric variables. However, no significant positive or negative correlation coefficients appear to exist among any pair of numerical variables. Therefore, there does not seem to be any issues regarding multicollinearity for potential models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb48338-7693-4049-b0e5-764ba21886a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = df_num_std.drop(columns=\"ID\").corr()\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"Greens\", fmt=\".3f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap between Numeric Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fde1ae-8947-48a1-89c1-c8e3592db4f6",
   "metadata": {},
   "source": [
    "## Bivariate Analysis - Chi-Square testing\n",
    "\n",
    "We can use `df_var` to perform Chi-Square testing to examine the relationship between two categorical variables. We will use this test to determine if there is a significant association between a pair of categorical variables based on the observed and expected frequencies in a contingency table. Since there are 10 categorical variables, there are 10 choose 2 (45) total combinations.\n",
    "\n",
    "We can see that only one result is significant at the significane level of 0.05, but since there are so many tests in total and 5% should be significant from chance alone, meaning 1 or 2 results appearing statistically significant out of chance is expected, so this likely holds little meaning without further analysis. This result was found between road condition and driver age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6aac92-2392-4a4c-a652-a5af7bebfc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical variables\n",
    "categorical_columns = df_var.drop(columns=\"ID\").columns\n",
    "\n",
    "# Perform Chi-square tests for each pair of categorical variables\n",
    "for i in range(len(categorical_columns)):\n",
    "    for j in range(i+1, len(categorical_columns)):\n",
    "        # Make contigency table and perform test\n",
    "        contingency_table = pd.crosstab(df_var[categorical_columns[i]], df_var[categorical_columns[j]])\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"Chi-square test between {categorical_columns[i]} and {categorical_columns[j]}:\")\n",
    "        print(f\"Chi-square statistic: \\t{chi2:.2f}\")\n",
    "        \n",
    "        flag = \"***\" if p_value < 0.05 else \"\" # Flag significant results\n",
    "        print(f\"P-value: \\t\\t{p_value:.2f}{flag}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712b8b6-6a6b-4a22-8a0e-de82ec5291e2",
   "metadata": {},
   "source": [
    "## Dimension Reduction through Principal Component Analysis (PCA)\r\n",
    "\r\n",
    "We are planning to conduct PCA for several reasons. Firstly, the da taset resulting from one-hot encoding combined with the numeric da taset will contain a total of 110 variables (after dropping `ID`), which would lead to many challenges in analysis due to its high dimensionality. PCA enables us to identify the most significant features within the da taset, leading to a clearer understanding and interpretation of relationships. Past that, PCA helps in reducing redundant data, streamlining the da taset for further analysis.\r\n",
    "\r\n",
    "Furthermore, PCA aids in preparing the data for modeling, working to fix issues and challenges such as overfitting, excessive complexity, an overly high dimensionality. By reducing the number of dimensions, PCA can improve the generalization ability of models and enhance computational efficiency.\r\n",
    "\r\n",
    "Upon visual inspection the cumulative variance explain seems to increase very slow for this data set. Depending on the type of analysis conducted using this data set, greater or fewer principle components would be desired. A data frame where the cumulative variance explained is at least 70% is saved, but the code could be easily modby changing the `desired_cve` variable ified for the data frame to ctained 80%, 90%, 95%, etc. cumulative variance explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be01227-5dce-4bc2-8072-569e0ea379c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction\n",
    "\n",
    "## Combine data and scale\n",
    "df_combined = pd.merge(df_var_encoded, df_num_std, on=\"ID\").drop(columns=\"ID\") # ID does not add to the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_combined)\n",
    "\n",
    "## Apply PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "## Find cumulative variance explained\n",
    "cumulative_variance_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "## Plot cumulative variance explained\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.lineplot(x=range(1, len(cumulative_variance_explained) + 1), y=cumulative_variance_explained, marker=\"o\", color=\"forestgreen\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Variance Explained\")\n",
    "plt.title(\"Cumulative Variance Explained by Principal Components\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## Save data frame of desired cumulative variance explained\n",
    "desired_cve = 0.7\n",
    "index_desired_percent = np.where(cumulative_variance_explained >= desired_cve)[0][0]\n",
    "pca_df = pd.DataFrame(principal_components[:, :index_desired_percent])\n",
    "\n",
    "## Fix column names\n",
    "pca_column_names = [f\"PC{i}\" for i in range(1, index_desired_percent+1)]\n",
    "pca_df.columns = pca_column_names\n",
    "display(pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc457fc-95f4-49b2-9f1f-2b4663c02349",
   "metadata": {},
   "source": [
    "## Modeling Implications\n",
    "\n",
    "Using principle component analysis only on the numeric variables, we can examine that while using fewer variables, a data frame can still contain at least 70% of the cumulative variance explained of the original data. This allows pairwise relationships to be easier to examine with less variables overall. Upon visual inspection, the pairwise plots reveal what appears to be random scatter and no discernible relationships between the pairs of variables. This data is in agreement with the results of the correlation matrix from before.\n",
    "\n",
    "Past that, with the Chi-square testing already done, there seems to be minimal, if any, apparent pair-wise relationships between categorical variables. This makes apparent that focus needs to be taken on individual contributions to the data over inter-variable relationships, and likely further analysis and more feature engineering would need to be done to derive insights for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4aa79-a142-4dd2-83d8-4fba9cf54760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pairwise relationships using scatterplots\n",
    "\n",
    "## First apply dimensional reduction\n",
    "## Combine data and scale\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_num_std)\n",
    "\n",
    "## Apply PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "## Find cumulative variance explained\n",
    "cumulative_variance_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "## Save data frame of desired cumulative variance explained\n",
    "desired_cve = 0.7\n",
    "index_desired_percent = np.where(cumulative_variance_explained >= desired_cve)[0][0]\n",
    "pca_df = pd.DataFrame(principal_components[:, :index_desired_percent])\n",
    "\n",
    "## Fix column names\n",
    "pca_column_names = [f\"PC{i}\" for i in range(1, index_desired_percent+1)]\n",
    "pca_df.columns = pca_column_names\n",
    "\n",
    "## Show pairwise relationships\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette([\"forestgreen\"])\n",
    "sns.pairplot(pca_df)\n",
    "plt.suptitle(\"Pairwise Relationships between Numeric Variables After Applying PCA\", fontsize=30, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440581c-bc68-44a3-a228-f85f20e073fc",
   "metadata": {},
   "source": [
    "## Modeling Implications continued\n",
    "\n",
    "If we try an ordinary least-squares model where number of casualties is the response variable, we can see the results for this model using all the other variables as predictors lead to extremely small R-squared and adjusted R-squared values, and it is not a statistically significant model by the probability of the F-statistics. This is just one example of a potential response variable, but it does not seem like modeling with this data set would be simple.\n",
    "\n",
    "Further feature engineering could be explored, but that is not the scope of this report. This could include adding potentially new variables from other existing variables, e.g. the time of day convert be converted to categories of afternoon, morning, night, etc. This could also involve exploring possibilities of interactions, but there are no current leads on which interactions could be valuable given the lack of apparent ways these variables interact that has been explored thus far.\n",
    "\n",
    "Depending on what type of modeling is required, further analysis could be conducted in numerous ways. Non-linear interactions could be explored through machine learning techniques such as random forest regression along with the transformations of variables to lead to better fitting of models. Experts and their research could be consulted to understand potentially useful combinations of variables, transformations of variables, which variables would need to be included in the final model, features to create, more information to gather related to these crashes potentially to add to the data, and there are many other possibilites as well to work towards modeling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bfee1-afdb-449b-87b6-c85410287123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling Implications continued\n",
    "\n",
    "## Define X and y\n",
    "y = df[\"Num_Casualties\"]\n",
    "X = df_combined.drop(columns=[\"Num_Casualties\"])\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "## Create and fit linear model\n",
    "model = sm.OLS(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "## Show results\n",
    "display(result.summary().tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122ba98-c843-474b-b075-9a3b7b568ec2",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This analysis includes several important steps on the road accident data set provided by Farshid Bahrami on Kaggle including: exploratory data analysis (EDA), data cleaning, visualization, feature engineering, dimensional reduction, bivariate analysis, and modeling implications. Here is a summary of the key findings from this analysis:\n",
    "\n",
    "1. Data Overview: The road accident data set contains information about various attributes related to road accidents across the United States, such as date, time, weather conditions, type of junction, vehicle type, and casualty counts. There are 1610 observations in total across 23 variables, 10 being categorical and 13 being numeric.\n",
    "2. Data Cleaning: This initially involved imputing the median values for numeric variables and the mode values for categorical variables. However, this was found to disrupt the nature of the distributions for the variables with imputed data, so more nuanced imputation method were selected instead. Namely, random forest classifying was selected for the categorical variables and interpolation was selected for numerical variables.  \n",
    "3. Feature Engineering: Time variables were transformed into numeric forms by the date variable being converted into days since the first observation was recorded and time of day was converted into hours since midnight. One-hot encoding was conducted on categorical variables to prepare the data for modeling. Standardization of numeric variables was done to prepare the data for modeling. A final data set was created to combine the standardized numeric data with the one-hot-encoded categorical variables.\n",
    "4. EDA: This involved visualizing numeric and categorical data to attempt to identify patterns in the data. This is where spikes in data where data was imputed initially was found, allowing this issue to be fixed. Histograms were used to examine numeric data sets and bar graphs were used for categorical variables. The variables appear to resemble roughly uniform distributions scaled differently to each variable. The data was found to not have any outliers or apparent errors and seemed to be ready to be prepared for modeling and further analysis.\n",
    "5. Bivariate Analysis: This was conducted using correlation matrices, pairwise plots, and conducting Chi-square tests across all pairs of categorical variables. Overall, no correlation matricies had any results that were not extremely weak, random scatter resulted from pairwise plots, and only one pair of categorical variables were found to be associated, but given there were 45 tests conducted overall, this is actually less than the expected result just based on random chance. Overall, the variables appear to be very unrelated to each other without further analysis.\n",
    "6. Dimensionality Reduction: Principle component analysis was employed to reduce the dimensionality of the data set while retaining the most significant information. The cumulative variance explained was examined through plotting and further analysis to be able to form data frames with optimal numbers of principle components.\n",
    "7. Modeling Implications: Linear regression was attempted using ordinary least-squares (OLS) regression with the response variable as number of casualties. However, the model did not yield any significant results. This seems to indcate that predicting using these available variables would be challenging for this response variable. For modeling desires, more feature engineering could be explored as well as more advanced machine learning techniques such as random forest regression to find non-linear interactions of variables in data for better model performance. Experts and their research could be consulted that could have significant implications for this type of modeling.\n",
    "\n",
    "In conclusion, this analysis demonstrates the importance of data preproccessing, explorary data analysis, and thoughtful model approaches in extracting insights from complex data sets. Further research, model exploration, and analysis could lead to valuable insights about this data, potentially for reducing road accident death, improving road safety measures, and improving road design."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
